{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "VocalSound.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyOpbG1V7/e90rXTrgPK73ZU",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/YuanGongND/vocalsound/blob/main/colab/VocalSound.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [VocalSound: A Dataset for Improving Human Vocal Sounds Recognition](https://ieeexplore.ieee.org/document/9746828)\n",
    "\n",
    "- This colab script contains the official code of the data preparation and baseline experiment in the [ICASSP paper](https://ieeexplore.ieee.org/document/9746828).\n",
    "\n",
    "- Please cite our paper(s) if you find the VocalSound dataset and code useful. The first paper proposes introduces the VocalSound dataset and the second paper describes the training pipeline and model we used for the baseline experiment.\n",
    "\n",
    "  - ```\n",
    "@INPROCEEDINGS{gong_vocalsound,\n",
    "  author={Gong, Yuan and Yu, Jin and Glass, James},\n",
    "  booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n",
    "  title={Vocalsound: A Dataset for Improving Human Vocal Sounds Recognition},\n",
    "  year={2022},\n",
    "  pages={151-155},\n",
    "  doi={10.1109/ICASSP43922.2022.9746828}}\n",
    "```\n",
    " - ```\n",
    "@ARTICLE{gong_psla,\n",
    "    author={Gong, Yuan and Chung, Yu-An and Glass, James},\n",
    "    title={PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation},\n",
    "    journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},\n",
    "    year={2021},\n",
    "    doi={10.1109/TASLP.2021.3120633}\n",
    "}\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 1. Prepare the VocalSound Data"
   ],
   "metadata": {
    "id": "2PNz6eTqi9iv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Step 1.1** Mount your Google Drive to this Colab as the VocalSound dataset is quite large (2.5 GB). You will be asked a few security check in this step."
   ],
   "metadata": {
    "id": "Ku2KqeDlnpyU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "data_dir = '/content/drive/MyDrive/vocalsound_baseline'\n",
    "if os.path.exists(data_dir) == True:\n",
    "    print('data path already exists')\n",
    "else:\n",
    "    os.mkdir(data_dir)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2yHBCFoiizQn",
    "outputId": "070c951d-1b9b-4db0-efcf-e7b37d3e74d9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "data path already exists\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Step 1.2** Download and unzip the VocalSound (16kHz version) dataset. Unzip process takes up to 20 minutes, please be patient."
   ],
   "metadata": {
    "id": "6Rw8qgsoxyS5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if os.path.exists('/content/drive/MyDrive/vocalsound_baseline/vs_release.zip') == False:\n",
    "  print('Downloading and uncompressing the VocalSound dataset, it takes up to 20 minutes, please be patient.')\n",
    "  os.system('wget https://www.dropbox.com/s/c5ace70qh1vbyzb/vs_release_16k.zip?dl=1 -O /content/drive/MyDrive/vocalsound_baseline/vs_release.zip')\n",
    "  os.system('unzip -q /content/drive/MyDrive/vocalsound_baseline/vs_release.zip -d /content/drive/MyDrive/vocalsound_baseline/')"
   ],
   "metadata": {
    "id": "fWLAo5altKn_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Step 1.3** Automatically correct the path in the data json file."
   ],
   "metadata": {
    "id": "kPgKOqrNoVT1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def get_immediate_files(a_dir):\n",
    "    return [name for name in os.listdir(a_dir) if os.path.isfile(os.path.join(a_dir, name))]\n",
    "\n",
    "def change_path(json_file_path, target_path):\n",
    "    with open(json_file_path, 'r') as fp:\n",
    "        data_json = json.load(fp)\n",
    "    data = data_json['data']\n",
    "\n",
    "    # change the path in the json file\n",
    "    for i in range(len(data)):\n",
    "        ori_path = data[i][\"wav\"]\n",
    "        new_path = target_path + '/audio_16k/' + ori_path.split('/')[-1]\n",
    "        data[i][\"wav\"] = new_path\n",
    "\n",
    "    with open(json_file_path, 'w') as f:\n",
    "        json.dump({'data': data}, f, indent=1)\n",
    "\n",
    "# for train, validation, test\n",
    "json_files = get_immediate_files(data_dir + '/datafiles/')\n",
    "for json_f in json_files:\n",
    "    if json_f.endswith('.json'):\n",
    "        print('now processing ' + data_dir + '/datafiles/' + json_f)\n",
    "        change_path(data_dir + '/datafiles/' + json_f, data_dir)\n",
    "\n",
    "# for subtest sets\n",
    "json_files = get_immediate_files(data_dir + '/datafiles/subtest/')\n",
    "for json_f in json_files:\n",
    "    if json_f.endswith('.json'):\n",
    "        print('now processing ' + data_dir + '/datafiles/subtest/' + json_f)\n",
    "        change_path(data_dir + '/datafiles/subtest/' + json_f, data_dir)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SPN2-DU2lrul",
    "outputId": "86aaa002-d0eb-4d8a-84a6-3c9a7a2897e3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "now processing /content/drive/MyDrive/vocalsound_baseline/datafiles/te.json\n",
      "now processing /content/drive/MyDrive/vocalsound_baseline/datafiles/val.json\n",
      "now processing /content/drive/MyDrive/vocalsound_baseline/datafiles/all.json\n",
      "now processing /content/drive/MyDrive/vocalsound_baseline/datafiles/tr.json\n",
      "now processing /content/drive/MyDrive/vocalsound_baseline/datafiles/subtest/te_male.json\n",
      "now processing /content/drive/MyDrive/vocalsound_baseline/datafiles/subtest/te_age3.json\n",
      "now processing /content/drive/MyDrive/vocalsound_baseline/datafiles/subtest/te_age2.json\n",
      "now processing /content/drive/MyDrive/vocalsound_baseline/datafiles/subtest/te_female.json\n",
      "now processing /content/drive/MyDrive/vocalsound_baseline/datafiles/subtest/te_age1.json\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 2. Define the EfficientNet Model"
   ],
   "metadata": {
    "id": "d7ht1S1MnYwR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can plug in and test your own model here. As long as your model's input and output is same with the baseline model, it should work with the script."
   ],
   "metadata": {
    "id": "BqyUsrnsKTKn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "class EffNetOri(torch.nn.Module):\n",
    "    def __init__(self, label_dim=6, pretrain=True, level=0):\n",
    "        super().__init__()\n",
    "        b = int(level)\n",
    "        if pretrain == True:\n",
    "            print('now train a effnet-b{:d} model with ImageNet pretrain'.format(b))\n",
    "        else:\n",
    "            print('now train a effnet-b{:d} model without ImageNet pretrain'.format(b))\n",
    "        if b == 7:\n",
    "            self.model = torchvision.models.efficientnet_b7(pretrained=pretrain)\n",
    "        elif b == 6:\n",
    "            self.model = torchvision.models.efficientnet_b6(pretrained=pretrain)\n",
    "        elif b == 5:\n",
    "            self.model = torchvision.models.efficientnet_b5(pretrained=pretrain)\n",
    "        elif b == 4:\n",
    "            self.model = torchvision.models.efficientnet_b4(pretrained=pretrain)\n",
    "        elif b == 3:\n",
    "            self.model = torchvision.models.efficientnet_b3(pretrained=pretrain)\n",
    "        elif b == 2:\n",
    "            self.model = torchvision.models.efficientnet_b2(pretrained=pretrain)\n",
    "        elif b == 1:\n",
    "            self.model = torchvision.models.efficientnet_b1(pretrained=pretrain)\n",
    "        elif b == 0:\n",
    "            self.model = torchvision.models.efficientnet_b0(pretrained=pretrain)\n",
    "\n",
    "        new_proj = torch.nn.Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        print('conv1 get from pretrained model.')\n",
    "        new_proj.weight = torch.nn.Parameter(torch.sum(self.model.features[0][0].weight, dim=1).unsqueeze(1))\n",
    "        new_proj.bias = self.model.features[0][0].bias\n",
    "        self.model.features[0][0] = new_proj\n",
    "        self.model = create_feature_extractor(self.model, {'features.8': 'mout'})\n",
    "        self.feat_dim, self.freq_dim = self.get_dim()\n",
    "        self.linear = nn.Linear(self.feat_dim, label_dim)\n",
    "\n",
    "    def get_dim(self):\n",
    "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        x = torch.zeros(10, 1000, 128)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.transpose(2, 3)\n",
    "        x = self.model(x)['mout']\n",
    "        return int(x.shape[1]), int(x.shape[2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.transpose(2, 3)\n",
    "        x = self.model(x)['mout']\n",
    "        x = torch.mean(x, dim=[2, 3])\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# # You can define your model here and test it with our pipeline.\n",
    "# # For simplicity, you can keep the model name same with us, and comment out our model.\n",
    "# class YourModel(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         # input in shape [batch_size, number of time frames, number of frequency bins], e.g., (12, 1024, 128)\n",
    "#         # your model here\n",
    "#         # output should be in shape [batch_size, 6] where 6 is the number of classes.\n",
    "#         return x"
   ],
   "metadata": {
    "id": "Jcot4PyreGWJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 3. Build the Dataloader"
   ],
   "metadata": {
    "id": "-9yKCZoC2Na5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import json\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "def make_index_dict(label_csv):\n",
    "    index_lookup = {}\n",
    "    with open(label_csv, 'r') as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            index_lookup[row['mid']] = row['index']\n",
    "            line_count += 1\n",
    "    return index_lookup\n",
    "\n",
    "def make_name_dict(label_csv):\n",
    "    name_lookup = {}\n",
    "    with open(label_csv, 'r') as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            name_lookup[row['index']] = row['display_name']\n",
    "            line_count += 1\n",
    "    return name_lookup\n",
    "\n",
    "def lookup_list(index_list, label_csv):\n",
    "    label_list = []\n",
    "    table = make_name_dict(label_csv)\n",
    "    for item in index_list:\n",
    "        label_list.append(table[item])\n",
    "    return label_list\n",
    "\n",
    "class VSDataset(Dataset):\n",
    "    def __init__(self, dataset_json_file, label_csv=None, audio_conf=None, raw_wav_mode=False, specaug=False):\n",
    "        self.datapath = dataset_json_file\n",
    "        with open(dataset_json_file, 'r') as fp:\n",
    "            data_json = json.load(fp)\n",
    "        self.data = data_json['data']\n",
    "        self.audio_conf = audio_conf\n",
    "        self.mode = self.audio_conf.get('mode')\n",
    "        self.melbins = self.audio_conf.get('num_mel_bins')\n",
    "        self.index_dict = make_index_dict(label_csv)\n",
    "        self.label_num = len(self.index_dict)\n",
    "        #print('Number of classes is {:d}'.format(self.label_num))\n",
    "\n",
    "        self.windows = {'hamming': scipy.signal.hamming, 'hann': scipy.signal.hann, 'blackman': scipy.signal.blackman, 'bartlett': scipy.signal.bartlett}\n",
    "\n",
    "        # if just load raw wavform\n",
    "        self.raw_wav_mode = raw_wav_mode\n",
    "        if specaug == True:\n",
    "            self.freqm = self.audio_conf.get('freqm')\n",
    "            self.timem = self.audio_conf.get('timem')\n",
    "            print('now using following mask: {:d} freq, {:d} time'.format(self.audio_conf.get('freqm'), self.audio_conf.get('timem')))\n",
    "        self.specaug = specaug\n",
    "        self.mixup = self.audio_conf.get('mixup')\n",
    "        #print('now using mix-up with rate {:f}'.format(self.mixup))\n",
    "        #print('now add rolling and new mixup stategy')\n",
    "\n",
    "    def _wav2fbank(self, filename, filename2=None):\n",
    "        # not mix-up, the colab version remove the mixup part\n",
    "        waveform, sr = torchaudio.load(filename)\n",
    "        waveform = waveform - waveform.mean()\n",
    "\n",
    "        fbank = torchaudio.compliance.kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr, use_energy=False,\n",
    "                                                  window_type='hanning', num_mel_bins=self.melbins, dither=0.0, frame_shift=10)\n",
    "\n",
    "        target_length = self.audio_conf.get('target_length', 512)\n",
    "        n_frames = fbank.shape[0]\n",
    "\n",
    "        p = target_length - n_frames\n",
    "\n",
    "        # cut and pad\n",
    "        if p > 0:\n",
    "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "            fbank = m(fbank)\n",
    "        elif p < 0:\n",
    "            fbank = fbank[0:target_length, :]\n",
    "\n",
    "        return fbank\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        datum = self.data[index]\n",
    "        label_indices = np.zeros(self.label_num) + 0.00\n",
    "        fbank = self._wav2fbank(datum['wav'])\n",
    "        for label_str in datum['labels'].split(','):\n",
    "            label_indices[int(self.index_dict[label_str])] = 1.0\n",
    "        label_indices = torch.FloatTensor(label_indices)\n",
    "\n",
    "        if self.specaug == True:\n",
    "            freqm = torchaudio.transforms.FrequencyMasking(self.freqm)\n",
    "            timem = torchaudio.transforms.TimeMasking(self.timem)\n",
    "            fbank = torch.transpose(fbank, 0, 1)\n",
    "            fbank = fbank.unsqueeze(0)\n",
    "            fbank = freqm(fbank)\n",
    "            fbank = timem(fbank)\n",
    "            fbank = fbank.squeeze(0)\n",
    "            fbank = torch.transpose(fbank, 0, 1)\n",
    "\n",
    "        # mean/std is get from the val set as a prior.\n",
    "        fbank = (fbank + 3.05) / 5.42\n",
    "\n",
    "        # shift if in the training set, training set typically use mixup\n",
    "        if self.mode == 'train':\n",
    "            fbank = torch.roll(fbank, np.random.randint(0, 1024), 0)\n",
    "\n",
    "        return fbank, label_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ],
   "metadata": {
    "id": "ndjF0akufmBo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 4. Define the Metrics"
   ],
   "metadata": {
    "id": "gZ98E5Um2XDi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy import stats as stats_func\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "\n",
    "def d_prime(auc):\n",
    "    standard_normal = stats_func.norm()\n",
    "    d_prime = standard_normal.ppf(auc) * np.sqrt(2.0)\n",
    "    return d_prime\n",
    "\n",
    "def calculate_stats(output, target):\n",
    "    \"\"\"Calculate statistics including mAP, AUC, etc.\n",
    "\n",
    "    Args:\n",
    "      output: 2d array, (samples_num, classes_num)\n",
    "      target: 2d array, (samples_num, classes_num)\n",
    "\n",
    "    Returns:\n",
    "      stats: list of statistic of each class.\n",
    "    \"\"\"\n",
    "\n",
    "    classes_num = target.shape[-1]\n",
    "    stats = []\n",
    "\n",
    "    # Class-wise statistics\n",
    "    for k in range(classes_num):\n",
    "\n",
    "        # Average precision\n",
    "        avg_precision = metrics.average_precision_score(\n",
    "            target[:, k], output[:, k], average=None)\n",
    "\n",
    "        # AUC\n",
    "        auc = metrics.roc_auc_score(target[:, k], output[:, k], average=None)\n",
    "\n",
    "        # Accuracy\n",
    "        # this is only used for single-label classification such as esc-50, not for multiple label one such as AudioSet\n",
    "        acc = metrics.accuracy_score(np.argmax(target, 1), np.argmax(output, 1))\n",
    "\n",
    "        # F1\n",
    "        target_i = np.argmax(target, axis=1)\n",
    "        output_i = np.argmax(output, axis=1)\n",
    "        f1 = metrics.f1_score(target_i, output_i, average=None)\n",
    "\n",
    "        # Precisions, recalls\n",
    "        (precisions, recalls, thresholds) = metrics.precision_recall_curve(\n",
    "            target[:, k], output[:, k])\n",
    "\n",
    "        # FPR, TPR\n",
    "        (fpr, tpr, thresholds) = metrics.roc_curve(target[:, k], output[:, k])\n",
    "\n",
    "        save_every_steps = 1     # Sample statistics to reduce size\n",
    "        dict = {'precisions': precisions[0::save_every_steps],\n",
    "                'recalls': recalls[0::save_every_steps],\n",
    "                'AP': avg_precision,\n",
    "                'fpr': fpr[0::save_every_steps],\n",
    "                'fnr': 1. - tpr[0::save_every_steps],\n",
    "                'auc': auc,\n",
    "                'acc': acc,\n",
    "                'f1': f1\n",
    "                }\n",
    "        stats.append(dict)\n",
    "\n",
    "    return stats"
   ],
   "metadata": {
    "id": "IYaXkWeMgdVO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 5. Create the Training and Evaluation Pipeline"
   ],
   "metadata": {
    "id": "tqaCKHkx2oA1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def train(audio_model, train_loader, test_loader, args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    torch.set_grad_enabled(True)\n",
    "    best_epoch, best_cum_epoch, best_mAP, best_acc, best_cum_mAP = 0, 0, -np.inf, -np.inf, -np.inf\n",
    "    global_step, epoch = 0, 0\n",
    "    exp_dir = args.exp_dir\n",
    "\n",
    "    audio_model = audio_model.to(device)\n",
    "    # Set up the optimizer\n",
    "    audio_trainables = [p for p in audio_model.parameters() if p.requires_grad]\n",
    "    print('Total parameter number is : {:.3f} million'.format(sum(p.numel() for p in audio_model.parameters()) / 1000000))\n",
    "    print('Total trainable parameter number is : {:.3f} million'.format(sum(p.numel() for p in audio_trainables) / 1000000))\n",
    "    trainables = audio_trainables\n",
    "\n",
    "    optimizer = torch.optim.Adam(trainables, args.lr, weight_decay=args.weight_decay, betas=(0.95, 0.999))\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, list(range(10, 60)), gamma=1.0)\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "    print(\"current #steps=%s, #epochs=%s\" % (global_step, epoch))\n",
    "    print(\"start training...\")\n",
    "\n",
    "    result = np.zeros([args.n_epochs, 9])\n",
    "    audio_model.train()\n",
    "    while epoch < args.n_epochs + 1:\n",
    "        audio_model.train()\n",
    "\n",
    "        for i, (audio_input, labels) in enumerate(train_loader):\n",
    "            # measure data loading time\n",
    "            B = audio_input.size(0)\n",
    "            audio_input = audio_input.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            audio_output = audio_model(audio_input)\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(audio_output, torch.argmax(labels.long(), axis=1))\n",
    "\n",
    "            # original optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            end_time = time.time()\n",
    "            global_step += 1\n",
    "\n",
    "        print('start validation')\n",
    "        stats, valid_loss = validate(audio_model, test_loader, args, epoch)\n",
    "\n",
    "        cum_stats = stats\n",
    "\n",
    "        cum_mAP = np.mean([stat['AP'] for stat in cum_stats])\n",
    "        cum_mAUC = np.mean([stat['auc'] for stat in cum_stats])\n",
    "        cum_acc = np.mean([stat['acc'] for stat in cum_stats])\n",
    "\n",
    "        mAP = np.mean([stat['AP'] for stat in stats])\n",
    "        mAUC = np.mean([stat['auc'] for stat in stats])\n",
    "        acc = np.mean([stat['acc'] for stat in stats])\n",
    "\n",
    "        middle_ps = [stat['precisions'][int(len(stat['precisions'])/2)] for stat in stats]\n",
    "        middle_rs = [stat['recalls'][int(len(stat['recalls'])/2)] for stat in stats]\n",
    "        average_precision = np.mean(middle_ps)\n",
    "        average_recall = np.mean(middle_rs)\n",
    "\n",
    "        print(\"---------------------Epoch {:d} Results---------------------\".format(epoch))\n",
    "        print(\"ACC: {:.6f}\".format(acc))\n",
    "        print(\"mAP: {:.6f}\".format(mAP))\n",
    "        print(\"AUC: {:.6f}\".format(mAUC))\n",
    "        print(\"Avg Precision: {:.6f}\".format(average_precision))\n",
    "        print(\"Avg Recall: {:.6f}\".format(average_recall))\n",
    "        print(\"d_prime: {:.6f}\".format(d_prime(mAUC)))\n",
    "        print(\"valid_loss: {:.6f}\".format(valid_loss))\n",
    "\n",
    "        result[epoch-1, :] = [mAP, acc, average_precision, average_recall, d_prime(mAUC), valid_loss, cum_mAP, cum_acc, optimizer.param_groups[0]['lr']]\n",
    "\n",
    "        np.savetxt(exp_dir + '/result.csv', result, delimiter=',')\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_acc_epoch = epoch\n",
    "            torch.save(audio_model.state_dict(), \"%s/models/best_audio_model.pth\" % (exp_dir))\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        #print('number of params groups:' + str(len(optimizer.param_groups)))\n",
    "        print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "def validate(audio_model, val_loader, args, epoch):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    audio_model = audio_model.to(device)\n",
    "    audio_model.eval()\n",
    "\n",
    "    A_predictions, A_targets, A_loss = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, (audio_input, labels) in enumerate(val_loader):\n",
    "            audio_input = audio_input.to(device)\n",
    "\n",
    "            # compute output\n",
    "            audio_output = audio_model(audio_input)\n",
    "            predictions = audio_output.to('cpu').detach()\n",
    "\n",
    "            A_predictions.append(predictions)\n",
    "            A_targets.append(labels)\n",
    "\n",
    "            # compute the loss\n",
    "            labels = labels.to(device)\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(audio_output, torch.argmax(labels.long(), axis=1))\n",
    "            A_loss.append(loss.to('cpu').detach())\n",
    "\n",
    "        audio_output = torch.cat(A_predictions)\n",
    "        target = torch.cat(A_targets)\n",
    "        loss = np.mean(A_loss)\n",
    "        stats = calculate_stats(audio_output, target)\n",
    "\n",
    "    return stats, loss"
   ],
   "metadata": {
    "id": "8YS3W5q-gfrg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 6. Train the EfficientNet model with VocalSound"
   ],
   "metadata": {
    "id": "Jdk7x6-n3Kp2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "print(\"I am process %s, running on %s: starting (%s)\" % (\n",
    "        os.getpid(), os.uname()[1], time.asctime()))\n",
    "\n",
    "# I/O args\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\"--data-train\", type=str, default='/content/drive/MyDrive/vocalsound_baseline/datafiles/tr.json', help=\"training data json\")\n",
    "parser.add_argument(\"--data-val\", type=str, default='/content/drive/MyDrive/vocalsound_baseline/datafiles/val.json', help=\"validation data json\")\n",
    "parser.add_argument(\"--label-csv\", type=str, default='/content/drive/MyDrive/vocalsound_baseline/class_labels_indices_vs.csv', help=\"csv with class labels\")\n",
    "parser.add_argument(\"--exp-dir\", type=str, default=\"/content/drive/MyDrive/vocalsound_baseline/baseline_exp/\", help=\"directory to dump experiments\")\n",
    "# training and optimization args\n",
    "parser.add_argument(\"--optim\", type=str, default=\"adam\", help=\"training optimizer\", choices=[\"sgd\", \"adam\"])\n",
    "parser.add_argument('-b', '--batch-size', default=80, type=int, metavar='N', help='mini-batch size (default: 100)')\n",
    "parser.add_argument('-w', '--num-workers', default=2, type=int, metavar='NW', help='# of workers for dataloading (default: 8)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=1e-4, type=float, metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--lr-decay', default=40, type=int, metavar='LRDECAY', help='Divide the learning rate by 10 every lr_decay epochs')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M', help='momentum')\n",
    "parser.add_argument('--weight-decay', '--wd', default=5e-7, type=float, metavar='W', help='weight decay (default: 1e-4)')\n",
    "parser.add_argument(\"--n-epochs\", type=int, default=20, help=\"number of maximum training epochs\")\n",
    "parser.add_argument(\"--n-print-steps\", type=int, default=1, help=\"number of steps to print statistics\")\n",
    "# models args\n",
    "parser.add_argument(\"--n_class\", type=int, default=6, help=\"number of classes\")\n",
    "parser.add_argument('--save_model', help='save the models or not', type=ast.literal_eval, default='False')\n",
    "parser.add_argument(\"--model\", type=str, default='eff_mean', help=\"model\")\n",
    "parser.add_argument(\"--model_size\", type=int, default=0, help=\"model size\")\n",
    "parser.add_argument('--imagenet_pretrain', help='if use pretrained imagenet efficient net', type=ast.literal_eval, default='False')\n",
    "parser.add_argument('--freqm', help='frequency mask max length', type=int, default=48)\n",
    "parser.add_argument('--timem', help='time mask max length', type=int, default=192)\n",
    "parser.add_argument(\"--mixup\", type=float, default=0, help=\"how many (0-1) samples need to be mixup during training\")\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "audio_conf = {'num_mel_bins': 128, 'target_length': 512, 'freqm': args.freqm, 'timem': args.timem, 'mixup': args.mixup, 'mode': 'train'}\n",
    "\n",
    "print('balanced sampler is not used')\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    VSDataset(args.data_train, label_csv=args.label_csv, audio_conf=audio_conf, raw_wav_mode=False, specaug=True),\n",
    "    batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
    "\n",
    "val_audio_conf = {'num_mel_bins': 128, 'target_length': 512, 'mixup': 0, 'mode': 'test'}\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    VSDataset(args.data_val, label_csv=args.label_csv, audio_conf=val_audio_conf, raw_wav_mode=False),\n",
    "    batch_size=200, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
    "\n",
    "if args.model == 'eff_mean':\n",
    "    audio_model = EffNetOri(label_dim=args.n_class, level=args.model_size, pretrain=args.imagenet_pretrain)\n",
    "else:\n",
    "    raise ValueError('Model Unrecognized')\n",
    "\n",
    "# start training\n",
    "if os.path.exists(args.exp_dir):\n",
    "    print(\"Deleting existing experiment directory %s\" % args.exp_dir)\n",
    "    shutil.rmtree(args.exp_dir)\n",
    "print(\"\\nCreating experiment directory: %s\" % args.exp_dir)\n",
    "os.makedirs(\"%s/models\" % args.exp_dir)\n",
    "with open(\"%s/args.pkl\" % args.exp_dir, \"wb\") as f:\n",
    "    pickle.dump(args, f)\n",
    "\n",
    "print('Now starting training for {:d} epochs'.format(args.n_epochs))\n",
    "train(audio_model, train_loader, val_loader, args)\n",
    "\n",
    "# test on the test set and sub-test set, model selected on the validation set\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sd = torch.load(args.exp_dir + '/models/best_audio_model.pth', map_location=device)\n",
    "audio_model.load_state_dict(sd)\n",
    "\n",
    "all_res = []\n",
    "\n",
    "# best model on the validation set, repeat to confirm\n",
    "stats, _ = validate(audio_model, val_loader, args, 'valid_set')\n",
    "# note it is NOT mean of class-wise accuracy\n",
    "val_acc = stats[0]['acc']\n",
    "val_mAUC = np.mean([stat['auc'] for stat in stats])\n",
    "print('---------------evaluate on the validation set---------------')\n",
    "print(\"Accuracy: {:.6f}\".format(val_acc))\n",
    "all_res.append(val_acc)\n",
    "\n",
    "# test the model on the evaluation set\n",
    "data_eval_list = ['te.json', 'subtest/te_age1.json', 'subtest/te_age2.json', 'subtest/te_age3.json', 'subtest/te_female.json', 'subtest/te_male.json']\n",
    "eval_name_list = ['all_test', 'test age 18-25', 'test age 26-48', 'test age 49-80', 'test female', 'test male']\n",
    "\n",
    "data_dir = '/'.join(args.data_val.split('/')[:-1])\n",
    "for idx, cur_eval in enumerate(data_eval_list):\n",
    "    cur_eval = data_dir + '/' + cur_eval\n",
    "    eval_loader = torch.utils.data.DataLoader(\n",
    "        VSDataset(cur_eval, label_csv=args.label_csv, audio_conf=val_audio_conf),\n",
    "        batch_size=args.batch_size*2, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
    "    stats, _ = validate(audio_model, eval_loader, args, eval_name_list[idx])\n",
    "    eval_acc = stats[0]['acc']\n",
    "    all_res.append(eval_acc)\n",
    "    print('---------------evaluate on {:s}---------------'.format(eval_name_list[idx]))\n",
    "    print(\"Accuracy: {:.6f}\".format(eval_acc))\n",
    "\n",
    "all_res = np.array(all_res)\n",
    "all_res = all_res.reshape([1, all_res.shape[0]])\n",
    "np.savetxt(args.exp_dir + '/all_eval_result.csv', all_res, header=','.join(['validation'] + eval_name_list), delimiter=',')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OL4uoFOKdpro",
    "outputId": "6b476a29-44a1-4cd6-bedf-7f3265aeb405"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I am process 71, running on 0dab978c433d: starting (Sat Apr 16 20:30:23 2022)\n",
      "balanced sampler is not used\n",
      "now using following mask: 48 freq, 192 time\n",
      "now train a effnet-b0 model without ImageNet pretrain\n",
      "conv1 get from pretrained model.\n",
      "Deleting existing experiment directory /content/drive/MyDrive/vocalsound_baseline/baseline_exp/\n",
      "\n",
      "Creating experiment directory: /content/drive/MyDrive/vocalsound_baseline/baseline_exp/\n",
      "Now starting training for 30 epochs\n",
      "cuda\n",
      "Total parameter number is : 4.015 million\n",
      "Total trainable parameter number is : 4.015 million\n",
      "current #steps=0, #epochs=1\n",
      "start training...\n",
      "start validation\n",
      "---------------------Epoch 1 Results---------------------\n",
      "ACC: 0.366038\n",
      "mAP: 0.371119\n",
      "AUC: 0.761857\n",
      "Avg Precision: 0.281954\n",
      "Avg Recall: 0.827373\n",
      "d_prime: 1.007327\n",
      "valid_loss: 1.491638\n",
      "number of params groups:1\n",
      "Epoch-1 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 2 Results---------------------\n",
      "ACC: 0.657143\n",
      "mAP: 0.676608\n",
      "AUC: 0.896861\n",
      "Avg Precision: 0.327757\n",
      "Avg Recall: 0.933077\n",
      "d_prime: 1.787376\n",
      "valid_loss: 0.939286\n",
      "number of params groups:1\n",
      "Epoch-2 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 3 Results---------------------\n",
      "ACC: 0.799461\n",
      "mAP: 0.844600\n",
      "AUC: 0.947153\n",
      "Avg Precision: 0.384537\n",
      "Avg Recall: 0.956853\n",
      "d_prime: 2.287994\n",
      "valid_loss: 0.605996\n",
      "number of params groups:1\n",
      "Epoch-3 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 4 Results---------------------\n",
      "ACC: 0.832884\n",
      "mAP: 0.898036\n",
      "AUC: 0.963035\n",
      "Avg Precision: 0.403382\n",
      "Avg Recall: 0.963319\n",
      "d_prime: 2.527264\n",
      "valid_loss: 0.523325\n",
      "number of params groups:1\n",
      "Epoch-4 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 5 Results---------------------\n",
      "ACC: 0.844205\n",
      "mAP: 0.912091\n",
      "AUC: 0.968456\n",
      "Avg Precision: 0.440058\n",
      "Avg Recall: 0.957394\n",
      "d_prime: 2.628407\n",
      "valid_loss: 0.470847\n",
      "number of params groups:1\n",
      "Epoch-5 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 6 Results---------------------\n",
      "ACC: 0.851752\n",
      "mAP: 0.921254\n",
      "AUC: 0.970224\n",
      "Avg Precision: 0.432602\n",
      "Avg Recall: 0.964424\n",
      "d_prime: 2.664508\n",
      "valid_loss: 0.450282\n",
      "number of params groups:1\n",
      "Epoch-6 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 7 Results---------------------\n",
      "ACC: 0.866307\n",
      "mAP: 0.920324\n",
      "AUC: 0.971151\n",
      "Avg Precision: 0.474547\n",
      "Avg Recall: 0.955757\n",
      "d_prime: 2.684148\n",
      "valid_loss: 0.417928\n",
      "number of params groups:1\n",
      "Epoch-7 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 8 Results---------------------\n",
      "ACC: 0.870620\n",
      "mAP: 0.923130\n",
      "AUC: 0.973044\n",
      "Avg Precision: 0.464557\n",
      "Avg Recall: 0.960655\n",
      "d_prime: 2.725966\n",
      "valid_loss: 0.400529\n",
      "number of params groups:1\n",
      "Epoch-8 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 9 Results---------------------\n",
      "ACC: 0.878167\n",
      "mAP: 0.929477\n",
      "AUC: 0.975404\n",
      "Avg Precision: 0.467225\n",
      "Avg Recall: 0.955764\n",
      "d_prime: 2.781643\n",
      "valid_loss: 0.366846\n",
      "number of params groups:1\n",
      "Epoch-9 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 10 Results---------------------\n",
      "ACC: 0.884636\n",
      "mAP: 0.934108\n",
      "AUC: 0.976385\n",
      "Avg Precision: 0.507792\n",
      "Avg Recall: 0.957414\n",
      "d_prime: 2.806140\n",
      "valid_loss: 0.371865\n",
      "number of params groups:1\n",
      "Epoch-10 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 11 Results---------------------\n",
      "ACC: 0.870081\n",
      "mAP: 0.930907\n",
      "AUC: 0.975641\n",
      "Avg Precision: 0.508225\n",
      "Avg Recall: 0.956314\n",
      "d_prime: 2.787492\n",
      "valid_loss: 0.380874\n",
      "number of params groups:1\n",
      "Epoch-11 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 12 Results---------------------\n",
      "ACC: 0.884636\n",
      "mAP: 0.933630\n",
      "AUC: 0.976821\n",
      "Avg Precision: 0.529163\n",
      "Avg Recall: 0.952550\n",
      "d_prime: 2.817282\n",
      "valid_loss: 0.349202\n",
      "number of params groups:1\n",
      "Epoch-12 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 13 Results---------------------\n",
      "ACC: 0.893261\n",
      "mAP: 0.931356\n",
      "AUC: 0.976970\n",
      "Avg Precision: 0.542327\n",
      "Avg Recall: 0.951465\n",
      "d_prime: 2.821121\n",
      "valid_loss: 0.339385\n",
      "number of params groups:1\n",
      "Epoch-13 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 14 Results---------------------\n",
      "ACC: 0.889488\n",
      "mAP: 0.938722\n",
      "AUC: 0.978089\n",
      "Avg Precision: 0.501638\n",
      "Avg Recall: 0.958454\n",
      "d_prime: 2.850746\n",
      "valid_loss: 0.346154\n",
      "number of params groups:1\n",
      "Epoch-14 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 15 Results---------------------\n",
      "ACC: 0.888410\n",
      "mAP: 0.938822\n",
      "AUC: 0.978296\n",
      "Avg Precision: 0.494459\n",
      "Avg Recall: 0.962778\n",
      "d_prime: 2.856370\n",
      "valid_loss: 0.358130\n",
      "number of params groups:1\n",
      "Epoch-15 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 16 Results---------------------\n",
      "ACC: 0.884636\n",
      "mAP: 0.933209\n",
      "AUC: 0.978441\n",
      "Avg Precision: 0.563689\n",
      "Avg Recall: 0.949820\n",
      "d_prime: 2.860346\n",
      "valid_loss: 0.362613\n",
      "number of params groups:1\n",
      "Epoch-16 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 17 Results---------------------\n",
      "ACC: 0.901348\n",
      "mAP: 0.935694\n",
      "AUC: 0.978708\n",
      "Avg Precision: 0.523419\n",
      "Avg Recall: 0.961163\n",
      "d_prime: 2.867708\n",
      "valid_loss: 0.326902\n",
      "number of params groups:1\n",
      "Epoch-17 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 18 Results---------------------\n",
      "ACC: 0.896496\n",
      "mAP: 0.927474\n",
      "AUC: 0.977914\n",
      "Avg Precision: 0.525086\n",
      "Avg Recall: 0.959573\n",
      "d_prime: 2.846053\n",
      "valid_loss: 0.340493\n",
      "number of params groups:1\n",
      "Epoch-18 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 19 Results---------------------\n",
      "ACC: 0.902965\n",
      "mAP: 0.937757\n",
      "AUC: 0.979273\n",
      "Avg Precision: 0.530391\n",
      "Avg Recall: 0.957946\n",
      "d_prime: 2.883532\n",
      "valid_loss: 0.326792\n",
      "number of params groups:1\n",
      "Epoch-19 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 20 Results---------------------\n",
      "ACC: 0.898652\n",
      "mAP: 0.936511\n",
      "AUC: 0.979528\n",
      "Avg Precision: 0.542709\n",
      "Avg Recall: 0.956335\n",
      "d_prime: 2.890785\n",
      "valid_loss: 0.328369\n",
      "number of params groups:1\n",
      "Epoch-20 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 21 Results---------------------\n",
      "ACC: 0.900809\n",
      "mAP: 0.930355\n",
      "AUC: 0.978544\n",
      "Avg Precision: 0.512164\n",
      "Avg Recall: 0.961188\n",
      "d_prime: 2.863179\n",
      "valid_loss: 0.319785\n",
      "number of params groups:1\n",
      "Epoch-21 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 22 Results---------------------\n",
      "ACC: 0.894879\n",
      "mAP: 0.938457\n",
      "AUC: 0.979272\n",
      "Avg Precision: 0.542188\n",
      "Avg Recall: 0.955797\n",
      "d_prime: 2.883499\n",
      "valid_loss: 0.336532\n",
      "number of params groups:1\n",
      "Epoch-22 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 23 Results---------------------\n",
      "ACC: 0.900809\n",
      "mAP: 0.930344\n",
      "AUC: 0.978944\n",
      "Avg Precision: 0.547672\n",
      "Avg Recall: 0.956330\n",
      "d_prime: 2.874276\n",
      "valid_loss: 0.315973\n",
      "number of params groups:1\n",
      "Epoch-23 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 24 Results---------------------\n",
      "ACC: 0.899730\n",
      "mAP: 0.934018\n",
      "AUC: 0.980330\n",
      "Avg Precision: 0.601482\n",
      "Avg Recall: 0.947160\n",
      "d_prime: 2.914147\n",
      "valid_loss: 0.318120\n",
      "number of params groups:1\n",
      "Epoch-24 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 25 Results---------------------\n",
      "ACC: 0.907817\n",
      "mAP: 0.936351\n",
      "AUC: 0.978977\n",
      "Avg Precision: 0.553791\n",
      "Avg Recall: 0.953610\n",
      "d_prime: 2.875197\n",
      "valid_loss: 0.315064\n",
      "number of params groups:1\n",
      "Epoch-25 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 26 Results---------------------\n",
      "ACC: 0.904582\n",
      "mAP: 0.942344\n",
      "AUC: 0.981177\n",
      "Avg Precision: 0.503756\n",
      "Avg Recall: 0.962245\n",
      "d_prime: 2.939715\n",
      "valid_loss: 0.315092\n",
      "number of params groups:1\n",
      "Epoch-26 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 27 Results---------------------\n",
      "ACC: 0.900809\n",
      "mAP: 0.916817\n",
      "AUC: 0.976255\n",
      "Avg Precision: 0.517208\n",
      "Avg Recall: 0.949307\n",
      "d_prime: 2.802833\n",
      "valid_loss: 0.331363\n",
      "number of params groups:1\n",
      "Epoch-27 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 28 Results---------------------\n",
      "ACC: 0.901887\n",
      "mAP: 0.939052\n",
      "AUC: 0.980851\n",
      "Avg Precision: 0.553500\n",
      "Avg Recall: 0.956340\n",
      "d_prime: 2.929771\n",
      "valid_loss: 0.322586\n",
      "number of params groups:1\n",
      "Epoch-28 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 29 Results---------------------\n",
      "ACC: 0.911590\n",
      "mAP: 0.944952\n",
      "AUC: 0.981240\n",
      "Avg Precision: 0.504738\n",
      "Avg Recall: 0.970888\n",
      "d_prime: 2.941637\n",
      "valid_loss: 0.317267\n",
      "number of params groups:1\n",
      "Epoch-29 lr: 0.0001\n",
      "start validation\n",
      "---------------------Epoch 30 Results---------------------\n",
      "ACC: 0.900270\n",
      "mAP: 0.940546\n",
      "AUC: 0.980460\n",
      "Avg Precision: 0.515476\n",
      "Avg Recall: 0.960636\n",
      "d_prime: 2.918000\n",
      "valid_loss: 0.310384\n",
      "number of params groups:1\n",
      "Epoch-30 lr: 0.0001\n",
      "---------------evaluate on the validation set---------------\n",
      "Accuracy: 0.911590\n",
      "---------------evaluate on all_test---------------\n",
      "Accuracy: 0.915622\n",
      "---------------evaluate on test age 18-25---------------\n",
      "Accuracy: 0.933857\n",
      "---------------evaluate on test age 26-48---------------\n",
      "Accuracy: 0.907699\n",
      "---------------evaluate on test age 49-80---------------\n",
      "Accuracy: 0.922414\n",
      "---------------evaluate on test female---------------\n",
      "Accuracy: 0.934272\n",
      "---------------evaluate on test male---------------\n",
      "Accuracy: 0.897806\n"
     ]
    }
   ]
  }
 ]
}